# RAG Template 2.0 Environment Configuration Template
# Copy this file to .env and fill in your actual values

# =============================================================================
# LLM Provider Configuration
# =============================================================================

# Choose your primary LLM provider: stackit, gemini, ollama
RAG_CLASS_TYPE_LLM_TYPE=stackit

# =============================================================================
# STACKIT vLLM Configuration (OpenAI-compatible API)
# =============================================================================

# STACKIT API credentials and configuration
STACKIT_VLLM_API_KEY=sk-your-stackit-api-key-here
STACKIT_VLLM_MODEL=cortecs/Llama-3.3-70B-Instruct-FP8-Dynamic
STACKIT_VLLM_BASE_URL=https://api.openai-compat.model-serving.eu01.onstackit.cloud/v1

# STACKIT LLM parameters
STACKIT_VLLM_TEMPERATURE=0.0
STACKIT_VLLM_TOP_P=0.1
STACKIT_VLLM_MAX_TOKENS=4096
STACKIT_VLLM_FREQUENCY_PENALTY=0.0
STACKIT_VLLM_PRESENCE_PENALTY=0.0

# =============================================================================
# Google Gemini Configuration
# =============================================================================

# Google AI API credentials and configuration
GEMINI_API_KEY=your-google-api-key-here
GEMINI_MODEL=gemini-1.5-pro

# Gemini LLM parameters
GEMINI_TEMPERATURE=0.7
GEMINI_MAX_OUTPUT_TOKENS=8192
GEMINI_TOP_P=0.95
GEMINI_TOP_K=40

# =============================================================================
# Ollama Local Configuration
# =============================================================================

# Ollama server configuration
OLLAMA_MODEL=llama3:instruct
OLLAMA_BASE_URL=http://localhost:11434

# Ollama LLM parameters
OLLAMA_TEMPERATURE=0.0
OLLAMA_TOP_K=0
OLLAMA_TOP_P=0.0

# =============================================================================
# Langfuse Configuration (Optional - for LLM observability)
# =============================================================================

# Langfuse tracing and monitoring
LANGFUSE_SECRET_KEY=your-langfuse-secret-key
LANGFUSE_PUBLIC_KEY=your-langfuse-public-key
LANGFUSE_HOST=https://cloud.langfuse.com

# =============================================================================
# RAG Pipeline Configuration
# =============================================================================

# Document processing and vectorization
EMBEDDING_MODEL=text-embedding-ada-002
VECTOR_STORE_TYPE=chroma
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Search and retrieval
TOP_K_DOCUMENTS=5
SIMILARITY_THRESHOLD=0.7

# =============================================================================
# Database Configuration
# =============================================================================

# PostgreSQL database configuration
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=rag_database
POSTGRES_USER=rag_user
POSTGRES_PASSWORD=your-postgres-password

# =============================================================================
# Application Configuration
# =============================================================================

# API and service configuration
API_PORT=8000
LOG_LEVEL=INFO
DEBUG=false

# CORS and security
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8080
JWT_SECRET_KEY=your-jwt-secret-key

# =============================================================================
# Tilt Development Configuration
# =============================================================================

# Development environment settings
TILT_CONTEXT=local
ENABLE_HOT_RELOAD=true
REBUILD_DEPENDENCIES=false

# Local development ports
FRONTEND_PORT=3000
BACKEND_PORT=8000
ADMIN_PORT=8001
EXTRACTOR_PORT=8002

# =============================================================================
# Notes and Usage Instructions
# =============================================================================

# 1. Provider Selection:
#    - Set RAG_CLASS_TYPE_LLM_TYPE to your preferred provider
#    - Configure the corresponding provider section above
#    - Only one provider needs to be configured at a time

# 2. STACKIT Configuration:
#    - Obtain API key from STACKIT console
#    - Use the provided base URL or your custom endpoint
#    - STACKIT uses OpenAI-compatible API format

# 3. Gemini Configuration:
#    - Obtain API key from Google AI Studio
#    - Choose from available models: gemini-1.5-pro, gemini-1.5-flash
#    - Configure output tokens and sampling parameters

# 4. Ollama Configuration:
#    - Ensure Ollama server is running locally
#    - Pull desired model: ollama pull llama3:instruct
#    - Adjust base URL if running on different host/port

# 5. Tilt Usage:
#    - Copy this file to .env: cp .env.template .env
#    - Fill in your actual values
#    - Run: tilt up
