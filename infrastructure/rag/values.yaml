features:
  ollama:
    enabled: false
  langfuse:
    enabled: true
  qdrant:
    enabled: true
  frontend:
    enabled: true
  keydb:
    enabled: true

backend:
  name: backend
  replicaCount: 1

  image:
    repository: schwarzit-xx-sit-rag-template-sit-stackit-docker-local.jfrog.io
    name: rag-backend
    pullPolicy: Always
    tag: "dev-ba20c7b"

  command:
  - "poetry"
  - "run"
  args:
  - "python"
  - "-m"
  - "uvicorn"
  - "main:perfect_rag_app"
  - "--host"
  - "0.0.0.0"
  - "--port"
  - "8080"
  - "--loop"
  - "asyncio"

  workers: 3
  wsMaxQueue: 6

  debugArgs:
  - "python"
  - "-m"
  - "debugpy"
  - "--wait-for-client"
  - "--listen"
  - "0.0.0.0:31415"
  - "-m"
  - "uvicorn"
  - "main:perfect_rag_app"
  - "--host"
  - "0.0.0.0"
  - "--port"
  - "8080"
  - "--reload"
  - "--reload-dir"
  - "/app/rag-backend"
  - "--reload-dir"
  - "/app/rag-core-library/rag-core-api"
  - "--reload-dir"
  - "/app/rag-core-library/rag-core-lib"
  - "--loop"
  - "asyncio"

  service:
    type: ClusterIP
    port: 8080

  pythonPathEnv:
    PYTHONPATH: src

  ingress:
    enabled: true
    host:
      name: rag.localhost
      path: /api/chat(/|$)(.*)
      pathType: ImplementationSpecific
      port: 8080

  secrets:
    basicAuth: ""
    langfuse:
      publicKey: "pk-lf"
      secretKey: "sk-lf"
    stackitEmbedder:
      apiKey: ""
    stackitVllm:
      apiKey: "sk-123"
    ragas:
      openaiApikey: ""

  envs:
    stackitVllm:
      STACKIT_VLLM_MODEL: neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8
      STACKIT_VLLM_BASE_URL: https://api.openai-compat.model-serving.eu01.onstackit.cloud/v1
    database:
      VECTOR_DB_COLLECTION_NAME: rag-db
      VECTOR_DB_LOCATION: http://rag-qdrant:6333
      VECTOR_DB_VALIDATE_COLLECTION_CONFIG: false
    retriever:
      RETRIEVER_THRESHOLD: 0.3
      RETRIEVER_K_DOCUMENTS: 10
      RETRIEVER_TOTAL_K: 7
      RETRIEVER_SUMMARY_THRESHOLD: 0.3
      RETRIEVER_SUMMARY_K_DOCUMENTS: 10
      RETRIEVER_TABLE_THRESHOLD: 0.3
      RETRIEVER_TABLE_K_DOCUMENTS: 10
      RETRIEVER_IMAGE_THRESHOLD: 0.7
      RETRIEVER_IMAGE_K_DOCUMENTS: 10
    errorMessages:
      ERROR_MESSAGES_NO_DOCUMENTS_MESSAGE: "I'm sorry, my responses are limited. You must ask the right questions."
      ERROR_MESSAGES_NO_OR_EMPTY_COLLECTION: "No documents were provided for searching."
      ERROR_MESSAGES_HARMFUL_QUESTION: "I'm sorry, but harmful requests cannot be processed."
      ERROR_MESSAGES_NO_ANSWER_FOUND: "I'm sorry, I couldn't find an answer with the context provided."
    langfuse:
      LANGFUSE_DATASET_NAME: "rag_test_ds"
      LANGFUSE_DATASET_FILENAME: "/app/test_data.json"
      LANGFUSE_HOST: "http://rag-langfuse:3000" #NOTE: http protocol needs to be defined!
    ragClassTypes:
      RAG_CLASS_TYPE_LLM_TYPE: "stackit"
    ragas:
      RAGAS_IS_DEBUG: false
      RAGAS_MODEL: "gpt-4o-mini"
      RAGAS_USE_OPENAI: true
      RAGAS_TIMEOUT: 60
      RAGAS_EVALUATION_DATASET_NAME: "eval-data"
      RAGAS_MAX_CONCURRENCY: "5"
    embedderClassTypes:
      EMBEDDER_CLASS_TYPE_EMBEDDER_TYPE: "stackit"
    stackitEmbedder:
      STACKIT_EMBEDDER_MODEL: "intfloat/e5-mistral-7b-instruct"
      STACKIT_EMBEDDER_BASE_URL: https://api.openai-compat.model-serving.eu01.onstackit.cloud/v1
    ollama:
      OLLAMA_MODEL: "llama3.2:3b-instruct-fp16"
      OLLAMA_BASE_URL: "http://rag-ollama:11434"
      OLLAMA_TOP_K: 0
      OLLAMA_TOP_P: 0
      OLLAMA_TEMPERATURE: 0
    ollamaEmbedder:
      OLLAMA_EMBEDDER_MODEL: "bge-m3"
      OLLAMA_EMBEDDER_BASE_URL: "http://rag-ollama:11434"
    fakeEmbedder:
      FAKE_EMBEDDER_SIZE: 386
    reranker:
      RERANKER_K_DOCUMENTS: 5
      RERANKER_MIN_RELEVANCE_SCORE: 0.001
    chatHistory:
      CHAT_HISTORY_LIMIT: 4
      CHAT_HISTORY_REVERSE: true

frontend:
  name: frontend
  replicaCount: 1
  image:
    repository: schwarzit-xx-sit-rag-template-sit-stackit-docker-local.jfrog.io
    name: frontend
    pullPolicy: Always
    tag: "dev-e12f8cc"

  service:
    type: ClusterIP
    port: 8080

  config:
    envs:
      VITE_API_URL: http://rag.localhost/api
      VITE_CHAT_URL: http://rag.localhost

  ingress:
    enabled: true
    host:
      name: rag.localhost
      path: /
      pathType: ImplementationSpecific
      port: 8080

  secrets:
    viteAuth:
      VITE_AUTH_USERNAME: ""
      VITE_AUTH_PASSWORD: ""

  envs:
    vite:
      VITE_CHAT_AUTH_ENABLED: true
      VITE_API_URL: "http://rag.localhost/api"
      VITE_CHAT_URL: "http://rag.localhost"
      VITE_ADMIN_URL: "http://admin.rag.localhost"
      VITE_ADMIN_API_URL: "http://admin.rag.localhost/api"

adminBackend:
  replicaCount: 1

  name: admin-backend

  image:
    repository: schwarzit-xx-sit-rag-template-sit-stackit-docker-local.jfrog.io
    name: admin-backend
    pullPolicy: Always
    tag: "443892"

  command:
  - "poetry"
  - "run"
  args:
  - "python"
  - "-m"
  - "uvicorn"
  - "main:perfect_admin_app"
  - "--host"
  - "0.0.0.0"
  - "--port"
  - "8080"
  - "--root-path"
  - "/api"
  debugArgs:
  - "python"
  - "-m"
  - "debugpy"
  - "--wait-for-client"
  - "--listen"
  - "0.0.0.0:31415"
  - "-m"
  - "uvicorn"
  - "main:perfect_admin_app"
  - "--host"
  - "0.0.0.0"
  - "--port"
  - "8080"
  - "--reload"
  - "--reload-dir"
  - "/app/admin-backend"
  - "--reload-dir"
  - "/app/rag-core-library/rag-core-lib"
  - "--reload-dir"
  - "/app/rag-core-library/admin-api-lib"
  - "--root-path"
  - "/api"

  service:
    type: ClusterIP
    port: 8080

  pythonPathEnv:
    PYTHONPATH: src

  ingress:
    enabled: true
    host:
      name: admin.rag.localhost
      path: /api(/|$)(.*)
      pathType: ImplementationSpecific
      port: 8080

  minio:
    enabled: true

  envs:
    summarizer:
      SUMMARIZER_MAXIMUM_INPUT_SIZE: "8000"
      SUMMARIZER_MAXIMUM_CONCURRENCY: "10"
    ragapi:
      RAG_API_HOST: "http://backend:8080"
    chunker:
      CHUNKER_MAX_SIZE: 1000
      CHUNKER_OVERLAP: 300
    confluenceLoader:
      CONFLUENCE_URL: ""
      CONFLUENCE_SPACE_KEY: ""
      CONFLUENCE_DOCUMENT_NAME: ""
    keyValueStore:
      USECASE_KEYVALUE_PORT: 6379
      USECASE_KEYVALUE_HOST: "rag-keydb"

  secrets:
    confluenceLoader:
      token: ""

extractor:
  replicaCount: 1
  name: extractor
  image:
    repository: schwarzit-xx-sit-rag-template-sit-stackit-docker-local.jfrog.io
    name: document-extractor
    pullPolicy: Always
    tag: "443892"

  command:
  - "poetry"
  - "run"
  args:
  - "python"
  - "-m"
  - "uvicorn"
  - "main:perfect_extractor_app"
  - "--host"
  - "0.0.0.0"
  - "--port"
  - "8080"
  debugArgs:
  - "python"
  - "-m"
  - "debugpy"
  - "--wait-for-client"
  - "--listen"
  - "0.0.0.0:31415"
  - "-m"
  - "uvicorn"
  - "main:perfect_extractor_app"
  - "--host"
  - "0.0.0.0"
  - "--port"
  - "8080"
  - "--reload"
  - "--reload-dir"
  - "/app/document-extractor"
  - "--reload-dir"
  - "/app/rag-core-library/extractor-api-lib"

  service:
    type: ClusterIP
    port: 8080

  pythonPathEnv:
    PYTHONPATH: src

adminFrontend:
  name: admin-frontend
  replicaCount: 1
  image:
    repository: schwarzit-xx-sit-rag-template-sit-stackit-docker-local.jfrog.io
    name: admin-frontend
    pullPolicy: Always
    tag: "443892"

  service:
    type: ClusterIP
    port: 8080

  exports:
    chart_name:
      adminFrontendChartName: admin-frontend

  ingress:
    enabled: true
    host:
      name: admin.rag.localhost
      path: /
      pathType: ImplementationSpecific
      port: 8080

shared:
  # These values are used across all templates
  ssl: true

  debug:
    backend:
      enabled: false

  imagePullSecret:
    # create: false
    # name: cr-credentials
    # auths:
    #   username: github-username # replace with your github username
    #   pat: github-pat # replace with your github personal access token
    #   email: email-address@domain.de # replace with your email address
    #   registry: ghcr.io

  config:
    dns:
    - rag.localhost
    - admin.rag.localhost
    basicAuth:
      enabled: true
    tls:
      enabled: true
      host: rag.localhost
      secretName: tls-certificate

  secrets:
    s3:
      accessKey: "admin"
      secretKey: "password"
    usecase:


  envs:
    pdfExtractor:
      PDF_EXTRACTOR_DIAGRAMS_FOLDER_NAME: "connection_diagrams"
      PDF_EXTRACTOR_FOOTER_HEIGHT: 155
    s3:
      S3_ENDPOINT: http://rag-minio:9000
      S3_BUCKET: documents
    usecase:


langfuse:
  image:
    repository: ghcr.io/langfuse/langfuse
    pullPolicy: Always
    tag: "2.93.0"
  postgresql:
    deploy: true
    auth:
      username: postgres
      password: postgres
      database: langfuse
  langfuse:
    nextauth:
      url: http://localhost:3000
      secret: changeme
    salt: changeme
    additionalEnv:
    - name: LANGFUSE_INIT_ORG_ID
      value: ""
    - name: LANGFUSE_INIT_PROJECT_ID
      value: ""
    - name: LANGFUSE_INIT_PROJECT_PUBLIC_KEY
      value: ""
    - name: LANGFUSE_INIT_PROJECT_SECRET_KEY
      value: ""
    - name: LANGFUSE_INIT_USER_EMAIL
      value: ""
    - name: LANGFUSE_INIT_USER_NAME
      value: ""
    - name: LANGFUSE_INIT_USER_PASSWORD
      value: ""
    extraInitContainers:
    - name: wait-for-postgres
      image: busybox
      command:
      - sh
      - -c
      - |
        until nc -z rag-postgresql 5432; do
          echo "Waiting for PostgreSQL to be ready..."
          sleep 2
        done
      # Define a reasonable timeout in case PostgreSQL fails to come up
      timeoutSeconds: 300

minio:
  auth:
    ## @param auth.rootUser MinIO&reg; root username
    ##
    rootUser: admin
    ## @param auth.rootPassword Password for MinIO&reg; root user
    ##
    rootPassword: "adminpassword"
  ## @param defaultBuckets Comma, semi-colon or space separated list of buckets to create at initialization (only in standalone mode)
  ## e.g:
  ## defaultBuckets: "my-bucket, my-second-bucket"
  ##
  defaultBuckets: "documents"

ollama:
  image:
    tag: 0.5.1
  ollama:
    models:
      pull:
        - llama3.2:3b-instruct-fp16
        - bge-m3
      runs:
        - llama3.2:3b-instruct-fp16
        - bge-m3

qdrant:
  image:
    tag: v1.12.6
